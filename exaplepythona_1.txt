import numpy as np
import os
import six.moves.urllib as urllib
import sys
import tarfile
import tensorflow as tf
import zipfile
import cv2

import PIL.ImageColor as ImageColor
import PIL.ImageDraw as ImageDraw
import PIL.ImageFont as ImageFont

from collections import defaultdict
from io import StringIO
from matplotlib import pyplot as plt
from PIL import Image
from keras.models import load_model
from keras.preprocessing import image
 
from object_detection.utils import label_map_util
 
from object_detection.utils import visualization_utils as vis_util

# function need to project 
def load_image_into_numpy_array(image):
    (im_width, im_height) = image.size;
    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)


def grayscale_image_load_data(path, gratmode=False, target_size=None):
    # muc tieu cua ham nay la viec minh lam mo tam anh xuong den muc toi thieu
    # dung dinh dang khuon mat de lay nhung box 
    # sau do lam mo den het muc co the 
    # tu do ta se dua vao data traning de co the biet cam xuc cua
    # moi box do la gi ?
    # muc tieu la lam sao co the nhan dien het tat ca khuon mat do
    # do cung la 1 ky thuat doi hoi su chinh xac
    print '------1 1------'
    print path
    pilgray_image = image.load_img(path, gratmode, target_size)
    return image.img_to_array(pilgray_image)

def preprocess_input(x, v2=True):
    x = x.astype('float32')
    x = x / 255.0
    if v2:
        x = x - 0.5
        x = x * 2.0
    return x

# function process label text for image
def get_labels(dataset_name):
    if dataset_name == 'fer2013':
        return {0:'angry',1:'disgust',2:'fear',3:'happy',
                4:'sad',5:'surprise',6:'neutral'}
    elif dataset_name == 'imdb':
        return {0:'woman', 1:'man'}
    elif dataset_name == 'KDEF':
        return {0:'AN', 1:'DI', 2:'AF', 3:'HA', 4:'SA', 5:'SU', 6:'NE'}
    else:
        raise Exception('Invalid dataset name')
# -------------------------





# loading detection emotion
emotion_model_path = './ssd_mobilenet_v1_coco_11_06_2017/fer2013_mini_XCEPTION.102-0.66.hdf5'
emotion_labels = get_labels('fer2013')
emotion_offsets = (20, 40)
emotion_offsets = (0, 0)
emotion_classifier = load_model(emotion_model_path, compile=False)
emotion_target_size = emotion_classifier.input_shape[1:3]
print emotion_target_size
# ---------------------------









PATH_TO_CKPT = "ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph_face.pb"
PATH_TO_LABELS = os.path.join('data', 'face_map_label.pbtxt')
NUM_CLASSES = 2
 
detection_graph = tf.Graph()
with detection_graph.as_default():
    od_graph_def = tf.GraphDef()
    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:
        serialized_graph = fid.read()
        od_graph_def.ParseFromString(serialized_graph)
        tf.import_graph_def(od_graph_def, name='')
#load label map
label_map = label_map_util.load_labelmap(PATH_TO_LABELS)
categories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)
category_index = label_map_util.create_category_index(categories)
 
#Detection
 
# For the sake of simplicity we will use only 2 images:
# image1.jpg
# image2.jpg
# If you want to test the code with your images, just add path to the images to the TEST_IMAGE_PATHS.
PATH_TO_TEST_IMAGES_DIR = 'test_images'
TEST_IMAGE_PATHS = [ os.path.join(PATH_TO_TEST_IMAGES_DIR, 'imageji{}.jpg'.format(i)) for i in range(1, 2) ]
 
# Size, in inches, of the output images.
IMAGE_SIZE = (12, 8)

# font word
font = ImageFont.truetype('Pillow/Tests/fonts/FreeMono.ttf', 40)
#font = ImageFont.truetype("open-sans/OpenSans-Regular.ttf", 8)

with detection_graph.as_default():
  with tf.Session(graph=detection_graph) as sess:
    for image_path in TEST_IMAGE_PATHS:
      imagesx = Image.open(image_path)
      print image_path
      # the array based representation of the image will be used later in order to prepare the
      # result image with boxes and labels on it.
      image_np = load_image_into_numpy_array(imagesx)
      # Expand dimensions since the model expects images to have shape: [1, None, None, 3]
      image_np_expanded = np.expand_dims(image_np, axis=0)
      
      
      
      # gray images 
      image_np_array1 = grayscale_image_load_data(image_path, gratmode=True)
      image_np_array1 = np.squeeze(image_np_array1)
      image_np_array1 = image_np_array1.astype('uint8')
      
      ## image_np_array1: la gray_image
      
      #-------------
      
      image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')
      # Each box reprunresents a part of the image where a particular object was detected.
      boxes = detection_graph.get_tensor_by_name('detection_boxes:0')
      # Each score represent how level of confidence for each of the objects.
      # Score is shown on the result image, together with the class label.
      scores = detection_graph.get_tensor_by_name('detection_scores:0')
      classes = detection_graph.get_tensor_by_name('detection_classes:0')
      num_detections = detection_graph.get_tensor_by_name('num_detections:0')
      # Actual detection.
      (boxes, scores, classes, num_detections) = sess.run(
          [boxes, scores, classes, num_detections],
          feed_dict={image_tensor: image_np_expanded})
      # Visualization of the results of a detection.
      vis_util.visualize_boxes_and_labels_on_image_array(
          image_np,
          np.squeeze(boxes),
          np.squeeze(classes).astype(np.int32),
          np.squeeze(scores),
          category_index,
          use_normalized_coordinates=True,
          line_thickness=8)
      
      
      width, height = imagesx.size
      draw = ImageDraw.Draw(imagesx)
      i = 0;
      for img_number in range(boxes.shape[1]):
          if np.squeeze(scores)[img_number] <= 0.5:
              continue
          ymin, xmin, ymax, xmax = boxes[0, img_number, :]
          i += 1
          print ('ymin:{} - xmin:{} - ymax:{} - xmax:{}', 
                 ymin, xmin, ymax, xmax)
          (left, right, top, bottom) = (xmin * width, xmax * width, ymin * height, ymax * height)
          left = left.astype(np.int32)
          right = right.astype(np.int32)
          top = top.astype(np.int32)
          bottom = bottom.astype(np.int32)
          print ('left:{} - right:{} - top:{} - bottom:{}', 
                 left, right, top, bottom)
          
          gray_face = image_np_array1[top:bottom, left:right] 
          cv2.imshow('hello', gray_face)
          cv2.waitKey(0)
          try:
              gray_face = cv2.resize(gray_face, (emotion_target_size))
          except:
              continue
          
          gray_face = preprocess_input(gray_face, True)
          print '1a'
          gray_face = np.expand_dims(gray_face, 0)
          print '2a'
          gray_face = np.expand_dims(gray_face, -1)
          print '3a'
          #emotion_label_arg = np.argmax(emotion_classifier.predict(gray_face))
          #emotion_label_hello = np.argmax(emotion_classifier.predict(gray_face))
          print '4a'
          emotion_text = emotion_labels[0]
          print emotion_text
          

          draw.rectangle([(left, top), (right, bottom)], outline='red')
          draw.text([(left, bottom)], emotion_text, (0, 255, 0), font=font)
          #draw.text((20, 70), "something123", font=ImageFont.truetype("font_path123"))

          
      imagesx.save('./test_images/rslx-1.jpg', "JPEG")
      print i
      
      plt.imshow(np.squeeze(image_np))
      plt.xticks([]), plt.yticks([])  # to hide tick values on X and Y axis
      plt.show()
      
      
      
      
